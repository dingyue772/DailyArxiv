---
title: Latest 5 Papers - April 13, 2025
labels: documentation
---
**Please check the [Github](https://github.com/dingyue772/DailyArxiv) page for a better reading experience and more papers.**

## Reward Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented Generation](http://arxiv.org/abs/2504.07794v1)** | 2025-04-10 |  |
| **[AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation](http://arxiv.org/abs/2504.07532v1)** | 2025-04-10 | Under Submission |
| **[Benchmarking Multimodal CoT Reward Model Stepwise by Visual Program](http://arxiv.org/abs/2504.06606v1)** | 2025-04-09 |  |
| **[Adversarial Training of Reward Models](http://arxiv.org/abs/2504.06141v1)** | 2025-04-08 | 16 pages, 7 figures |
| **[Information-Theoretic Reward Decomposition for Generalizable RLHF](http://arxiv.org/abs/2504.06020v1)** | 2025-04-08 | <details><summary>Work ...</summary><p>Work done during internships at Institute of Artificial Intelligence (TeleAI), China Telecom</p></details> |

## Reasoning
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation](http://arxiv.org/abs/2504.07962v1)** | 2025-04-10 | CVPR 2025 |
| **[VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning](http://arxiv.org/abs/2504.07956v1)** | 2025-04-10 |  |
| **[SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement](http://arxiv.org/abs/2504.07934v1)** | 2025-04-10 | 21 pages, 5 figures |
| **[SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning](http://arxiv.org/abs/2504.07891v1)** | 2025-04-10 |  |
| **[What the HellaSwag? On the Validity of Common-Sense Reasoning Benchmarks](http://arxiv.org/abs/2504.07825v1)** | 2025-04-10 |  |

