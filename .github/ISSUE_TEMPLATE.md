---
title: Latest 5 Papers - August 16, 2025
labels: documentation
---
**Please check the [Github](https://github.com/dingyue772/DailyArxiv) page for a better reading experience and more papers.**

## Hallucination
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs](http://arxiv.org/abs/2508.10264v1)** | 2025-08-14 |  |
| **[Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](http://arxiv.org/abs/2508.10192v1)** | 2025-08-13 | 24 pages, 3 figures |
| **[SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in LVLMs](http://arxiv.org/abs/2508.09584v2)** | 2025-08-14 |  |
| **[Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis](http://arxiv.org/abs/2508.09458v2)** | 2025-08-14 |  |
| **[Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics](http://arxiv.org/abs/2508.08661v1)** | 2025-08-12 | <details><summary>8 mai...</summary><p>8 main pages, 5 figures</p></details> |

## Safety
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Online Safety under Multiple Constraints and Input Bounds using gatekeeper: Theory and Applications](http://arxiv.org/abs/2508.09963v1)** | 2025-08-13 | <details><summary>6 pag...</summary><p>6 pages, 2 figures. Accepted for publication in IEEE L-CSS 2025</p></details> |
| **[The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](http://arxiv.org/abs/2508.09762v1)** | 2025-08-13 | <details><summary>10 pa...</summary><p>10 pages, 4 figures, 2 tables</p></details> |
| **[From Formal Methods to Data-Driven Safety Certificates of Unknown Large-Scale Networks](http://arxiv.org/abs/2508.09520v1)** | 2025-08-13 |  |
| **[NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](http://arxiv.org/abs/2508.09473v1)** | 2025-08-13 |  |
| **[How Safe Will I Be Given What I Saw? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy](http://arxiv.org/abs/2508.09346v1)** | 2025-08-12 |  |

