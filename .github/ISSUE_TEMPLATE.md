---
title: Latest 5 Papers - October 16, 2025
labels: documentation
---
**Please check the [Github](https://github.com/dingyue772/DailyArxiv) page for a better reading experience and more papers.**

## Hallucination
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models](http://arxiv.org/abs/2510.12137v1)** | 2025-10-14 |  |
| **[Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions](http://arxiv.org/abs/2510.12040v1)** | 2025-10-14 | <details><summary>24 pa...</summary><p>24 pages, 3 figures, magazine</p></details> |
| **[Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models](http://arxiv.org/abs/2510.12032v1)** | 2025-10-14 | 22 pages, 6 figures |
| **[CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement](http://arxiv.org/abs/2510.12029v1)** | 2025-10-14 | <details><summary>2024 ...</summary><p>2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 7 pages, 2 figures</p></details> |
| **[Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models](http://arxiv.org/abs/2510.11529v1)** | 2025-10-13 |  |

## Safety
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[SafeMT: Multi-turn Safety for Multimodal Language Models](http://arxiv.org/abs/2510.12133v1)** | 2025-10-14 |  |
| **[An AI-Based Behavioral Health Safety Filter and Dataset for Identifying Mental Health Crises in Text-Based Conversations](http://arxiv.org/abs/2510.12083v1)** | 2025-10-14 | <details><summary>Main ...</summary><p>Main Text: 2943; Abstract: 256; Tables and Figures: 5</p></details> |
| **[Task-Specific Dual-Model Framework for Comprehensive Traffic Safety Video Description and Analysis](http://arxiv.org/abs/2510.11907v1)** | 2025-10-13 | <details><summary>This ...</summary><p>This paper was accepted at ICCV 2025</p></details> |
| **[Bag of Tricks for Subverting Reasoning-based Safety Guardrails](http://arxiv.org/abs/2510.11570v1)** | 2025-10-13 | <details><summary>OpenA...</summary><p>OpenAI Red-teaming Challenge Winner and Oral Presentation</p></details> |
| **[AI Alignment Strategies from a Risk Perspective: Independent Safety Mechanisms or Shared Failures?](http://arxiv.org/abs/2510.11235v1)** | 2025-10-13 | under review |

