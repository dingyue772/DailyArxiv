---
title: Latest 5 Papers - September 25, 2025
labels: documentation
---
**Please check the [Github](https://github.com/dingyue772/DailyArxiv) page for a better reading experience and more papers.**

## Hallucination
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions](http://arxiv.org/abs/2509.18970v1)** | 2025-09-23 |  |
| **[Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications](http://arxiv.org/abs/2509.17671v1)** | 2025-09-22 |  |
| **[ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding](http://arxiv.org/abs/2509.17481v1)** | 2025-09-22 |  |
| **[Semantic Reformulation Entropy for Robust Hallucination Detection in QA Tasks](http://arxiv.org/abs/2509.17445v1)** | 2025-09-22 | <details><summary>5page...</summary><p>5pages, 5 figures, submit to ICASSP 2026</p></details> |
| **[Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach to Knowledge Retrieval and Hallucination Reduction](http://arxiv.org/abs/2509.16369v1)** | 2025-09-19 | <details><summary>14 Pa...</summary><p>14 Pages, 8 Tables, 2 Figures. Accepted and to be published in the proceedings of FinNLP, Empirical Methods in Natural Language Processing 2025</p></details> |

## Safety
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Steering Multimodal Large Language Models Decoding for Context-Aware Safety](http://arxiv.org/abs/2509.19212v1)** | 2025-09-23 | <details><summary>A lig...</summary><p>A lightweight and model-agnostic decoding framework that dynamically adjusts token generation based on multimodal context</p></details> |
| **[SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer](http://arxiv.org/abs/2509.18648v1)** | 2025-09-23 |  |
| **[Refined Barrier Conditions for Finite-Time Safety and Reach-Avoid Guarantees in Stochastic Systems](http://arxiv.org/abs/2509.18518v1)** | 2025-09-23 |  |
| **[Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints](http://arxiv.org/abs/2509.18382v1)** | 2025-09-22 |  |
| **[Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs](http://arxiv.org/abs/2509.18058v2)** | 2025-09-23 |  |

