---
title: Latest 5 Papers - October 22, 2025
labels: documentation
---
**Please check the [Github](https://github.com/dingyue772/DailyArxiv) page for a better reading experience and more papers.**

## Hallucination
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](http://arxiv.org/abs/2510.17733v1)** | 2025-10-20 |  |
| **[Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](http://arxiv.org/abs/2510.17210v1)** | 2025-10-20 | 22 pages, 10 figures |
| **[SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](http://arxiv.org/abs/2510.16596v1)** | 2025-10-18 |  |
| **[Hallucination Benchmark for Speech Foundation Models](http://arxiv.org/abs/2510.16567v1)** | 2025-10-18 | Under Review |
| **[Beyond Hallucinations: The Illusion of Understanding in Large Language Models](http://arxiv.org/abs/2510.14665v1)** | 2025-10-16 |  |

## Safety
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Modelling complexity in system safety: generalizing the D2T2 methodology](http://arxiv.org/abs/2510.17351v1)** | 2025-10-20 |  |
| **[Multimodal Safety Is Asymmetric: Cross-Modal Exploits Unlock Black-Box MLLMs Jailbreaks](http://arxiv.org/abs/2510.17277v1)** | 2025-10-20 |  |
| **[SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](http://arxiv.org/abs/2510.17017v2)** | 2025-10-21 |  |
| **[Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations](http://arxiv.org/abs/2510.16893v1)** | 2025-10-19 | <details><summary>Submi...</summary><p>Submitted to ICASSP 2026</p></details> |
| **[Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](http://arxiv.org/abs/2510.16492v1)** | 2025-10-18 | <details><summary>Relia...</summary><p>Reliable ML and Regulatable ML workshops, Neurips 2025</p></details> |

