---
title: Latest 5 Papers - October 12, 2025
labels: documentation
---
**Please check the [Github](https://github.com/dingyue772/DailyArxiv) page for a better reading experience and more papers.**

## Hallucination
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Revisiting Hallucination Detection with Effective Rank-based Uncertainty](http://arxiv.org/abs/2510.08389v1)** | 2025-10-09 |  |
| **[Detecting and Mitigating Insertion Hallucination in Video-to-Audio Generation](http://arxiv.org/abs/2510.08078v1)** | 2025-10-09 |  |
| **[The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs](http://arxiv.org/abs/2510.07775v1)** | 2025-10-09 |  |
| **[Injecting Hallucinations in Autonomous Vehicles: A Component-Agnostic Safety Evaluation Framework](http://arxiv.org/abs/2510.07749v1)** | 2025-10-09 | <details><summary>22 pa...</summary><p>22 pages, 15 figures, 21 tables</p></details> |
| **[Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models](http://arxiv.org/abs/2510.06107v2)** | 2025-10-08 |  |

## Safety
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[The Alignment Waltz: Jointly Training Agents to Collaborate for Safety](http://arxiv.org/abs/2510.08240v1)** | 2025-10-09 |  |
| **[The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs](http://arxiv.org/abs/2510.07775v1)** | 2025-10-09 |  |
| **[Injecting Hallucinations in Autonomous Vehicles: A Component-Agnostic Safety Evaluation Framework](http://arxiv.org/abs/2510.07749v1)** | 2025-10-09 | <details><summary>22 pa...</summary><p>22 pages, 15 figures, 21 tables</p></details> |
| **[Multimodal Safety Evaluation in Generative Agent Social Simulations](http://arxiv.org/abs/2510.07709v1)** | 2025-10-09 |  |
| **[SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models](http://arxiv.org/abs/2510.06871v2)** | 2025-10-09 |  |

